<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Analysis and visualisation of synchrony, interaction, and joint
    movements from audio and video movement data of a group of music performers. The demo is data described in Clayton, Leante, and 
    Tarsitani (2021) &lt;doi:10.17605/OSF.IO/KS325&gt;, while example analyses 
    can be found in Clayton, Jakubowski, and Eerola (2019) 
    &lt;doi:10.1177/1029864919844809&gt;. Additionally, wavelet analysis 
    techniques have been applied to examine movement-related 
    musical interactions, as shown in Eerola et al. (2018)
    &lt;doi:10.1098/rsos.171520&gt;.">
<title>Analysis and Visualisation of Musical Audio and Video Movement
    Synchrony Data • movementsync</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="apple-touch-icon-60x60.png">
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Analysis and Visualisation of Musical Audio and Video Movement
    Synchrony Data">
<meta property="og:description" content="Analysis and visualisation of synchrony, interaction, and joint
    movements from audio and video movement data of a group of music performers. The demo is data described in Clayton, Leante, and 
    Tarsitani (2021) &lt;doi:10.17605/OSF.IO/KS325&gt;, while example analyses 
    can be found in Clayton, Jakubowski, and Eerola (2019) 
    &lt;doi:10.1177/1029864919844809&gt;. Additionally, wavelet analysis 
    techniques have been applied to examine movement-related 
    musical interactions, as shown in Eerola et al. (2018)
    &lt;doi:10.1098/rsos.171520&gt;.">
<meta property="og:image" content="https://github.com/tuomaseerola/movementsync/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="index.html">movementsync</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.4</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header">
<img src="logo.png" class="logo" alt=""><h1 id="movementsync-analysis-and-visualisation-of-musical-audio-and-video-movement-synchrony-data">movementsync: Analysis and Visualisation of Musical Audio and Video Movement Synchrony Data<a class="anchor" aria-label="anchor" href="#movementsync-analysis-and-visualisation-of-musical-audio-and-video-movement-synchrony-data"></a>
</h1>
</div>
<!-- badges: start -->

<p>The goal of <code>movementsync</code> library is to provide analysis and visualisation of synchrony, interaction, and joint movements from audio and video movement data of a group of music performers. Functions in the library offer analysis routines for visualising, selecting, and filtering data. Analysis functions for carrying out Granger causality analysis and wavelet analysis are included. Routines for combining movement data, music instrument onsets, and annotations are also offered in the library.</p>
<p>For documentation, see <a href="https://tuomaseerola.github.io/movementsync/articles/movementsync.html" class="external-link">Quick Guide</a>.</p>
<p><strong>Note:</strong> <em>Movement data</em> is obtained through analysis of videos using suitable computer vision techniques (e.g. <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" class="external-link">openpose</a> or obtaining motion capture data. Similarly, <em>onset data</em> is extracted from audio recordings using onset detection algorithms. Also, the <em>annotations</em> of rhythmic structures, musical events, or musical form of an ensemble performances can be carried out in <a href="https://archive.mpi.nl/tla/elan" class="external-link">ELAN</a> or another annotation tool for audio and video. <code>movementsync</code> library does not deal with this initial extraction of pose from video or onsets from audio, but it offers a versatile suite of functions to analyse the extracted data.</p>
<p>The library supports the open data described in Clayton, Leante, and Tarsitani (2021) <a href="doi:10.17605/OSF.IO/KS325" class="uri">doi:10.17605/OSF.IO/KS325</a>. Example analyses can be found in Clayton, Jakubowski, and Eerola (2019) <a href="doi:10.1177/1029864919844809" class="uri">doi:10.1177/1029864919844809</a>. Wavelet analysis techniques applied to musical interactions have been reported in Eerola et al. (2018) <a href="doi:10.1098/rsos.171520" class="uri">doi:10.1098/rsos.171520</a>.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install <code>movementsync</code> from CRAN:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"movementsync"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-sample-data">Load sample data<a class="anchor" aria-label="anchor" href="#load-sample-data"></a>
</h2>
<p>Here we load a short demo data that comes with the package. This data is 1 min of video/feature data (1500 observations with 25 frames per second (fps) taken from North Indian Raga performance by <em>Anupama Bhagwat</em> (sitar) and <em>Gurdain Rayatt</em> (Tabla) performing <em>Rag Puriya</em> (Recording ID as <code>NIR_ABh_Puriya</code> available in <a href="https://osf.io/ks325/" class="external-link">OSF</a>. The first video frame of the performance is shown in the image below. You can use external datasets and our longer demonstrations for more extensive examples.</p>
<div class="figure">
<img src="reference/figures/puriya.png" alt=""><p class="caption">A frame of the video from which the pose estimation has been done</p>
</div>
<p>Get all markers of the sitar player and plot them.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">movementsync</span><span class="op">)</span></span>
<span><span class="va">r1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_sample_recording.html">get_sample_recording</a></span><span class="op">(</span><span class="op">)</span>                                                <span class="co"># Defaults to NIR_ABh_Puriya</span></span>
<span><span class="va">rv1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_raw_view.html">get_raw_view</a></span><span class="op">(</span><span class="va">r1</span>, <span class="st">"Central"</span>, <span class="st">""</span>, <span class="st">"Sitar"</span><span class="op">)</span>                             <span class="co"># Take the sitar player</span></span>
<span><span class="va">pv1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_processed_view.html">get_processed_view</a></span><span class="op">(</span><span class="va">rv1</span><span class="op">)</span></span>
<span><span class="va">dp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"LWrist"</span>,<span class="st">"RWrist"</span>,<span class="st">"LElbow"</span>,<span class="st">"RElbow"</span>,<span class="st">"LEye"</span>,<span class="st">"REye"</span>,<span class="st">"Neck"</span>,<span class="st">"MidHip"</span><span class="op">)</span>  <span class="co"># Define markers</span></span>
<span><span class="va">fv1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/apply_filter_sgolay.html">apply_filter_sgolay</a></span><span class="op">(</span><span class="va">pv1</span>, data_point <span class="op">=</span> <span class="va">dp</span>, n <span class="op">=</span> <span class="fl">41</span>, p <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>             <span class="co"># Apply smoothing</span></span>
<span><span class="fu"><a href="reference/distribution_dp.html">distribution_dp</a></span><span class="op">(</span><span class="va">fv1</span><span class="op">)</span>                                                        <span class="co"># Plot</span></span></code></pre></div>
<p><img src="reference/figures/README-ex1-1.png" width="70%"></p>
<p>Plot the Y coordinate of the nose marker from the sitar player.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">pv1</span>, columns <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Nose_y"</span><span class="op">)</span><span class="op">)</span>                        <span class="co"># Define markers/coords and plot</span></span></code></pre></div>
<p><img src="reference/figures/README-ex2-1.png" width="70%"></p>
<div class="section level3">
<h3 id="filtering">Filtering<a class="anchor" aria-label="anchor" href="#filtering"></a>
</h3>
<p>We usually want to filter raw movement data and here we use a Savitzy-Golay filter to smooth the data.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fv1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/apply_filter_sgolay.html">apply_filter_sgolay</a></span><span class="op">(</span><span class="va">pv1</span>, <span class="st">"Nose"</span>, n <span class="op">=</span> <span class="fl">81</span>, p <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>      <span class="co"># Filter with rather heavy parameters    </span></span>
<span><span class="fu"><a href="reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">fv1</span>, columns <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Nose_y"</span><span class="op">)</span><span class="op">)</span>                        <span class="co"># Define markers and coordinates and plot </span></span></code></pre></div>
<p><img src="reference/figures/README-ex3-1.png" width="70%"></p>
</div>
<div class="section level3">
<h3 id="annotation">Annotation<a class="anchor" aria-label="anchor" href="#annotation"></a>
</h3>
<p>Add an arbitrary annotation that contains three 20-second segments (A, B, and C).</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">l</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>A <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">20</span><span class="op">)</span>, B <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">40</span><span class="op">)</span>, C <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">40</span>, <span class="fl">60</span><span class="op">)</span><span class="op">)</span>         <span class="co"># Define three segments</span></span>
<span><span class="va">splicing_dfr</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/splice_time.html">splice_time</a></span><span class="op">(</span><span class="va">l</span><span class="op">)</span>                                <span class="co"># Create a splicing table</span></span>
<span><span class="fu"><a href="reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">fv1</span>, columns <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Nose_y"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>                        <span class="co"># Plot </span></span>
<span>  <span class="fu"><a href="reference/autolayer.html">autolayer</a></span><span class="op">(</span><span class="va">splicing_dfr</span><span class="op">)</span>                                     <span class="co"># add annotations to the plot  </span></span></code></pre></div>
<p><img src="reference/figures/README-ex4-1.png" width="70%"></p>
</div>
</div>
<div class="section level2">
<h2 id="granger-causality-analysis">Granger causality analysis<a class="anchor" aria-label="anchor" href="#granger-causality-analysis"></a>
</h2>
<p>Are the head movements of the two musicians related to each other? Is one of the musicians leading the movements and the other is following? To explore this, we can apply Granger causality analysis. Here we take the nose markers from both sitar and tabla players, apply a smoothing filter, and obtain a combined view of this data.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fv_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_filtered_views.html">get_filtered_views</a></span><span class="op">(</span><span class="va">r1</span>, data_points <span class="op">=</span> <span class="st">"Nose"</span>, n <span class="op">=</span> <span class="fl">41</span>, p <span class="op">=</span><span class="fl">3</span><span class="op">)</span> <span class="co"># Filter</span></span>
<span><span class="va">jv</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_joined_view.html">get_joined_view</a></span><span class="op">(</span><span class="va">fv_list</span><span class="op">)</span>                                        <span class="co"># Combine the data</span></span></code></pre></div>
<div class="section level3">
<h3 id="time-splices">Time splices<a class="anchor" aria-label="anchor" href="#time-splices"></a>
</h3>
<p>Next we split our data into 30-second segments with a 6-second step size to create segments to be tested with the Granger causality analysis. This is an extremely blunt way of performing the analysis but illustrates the procedure.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">splicing_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/splice_time.html">splice_time</a></span><span class="op">(</span><span class="va">jv</span>, win_size <span class="op">=</span> <span class="fl">30</span>, step_size <span class="op">=</span> <span class="fl">6</span><span class="op">)</span>            <span class="co"># Splice into segments</span></span>
<span><span class="va">sv</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_spliced_view.html">get_spliced_view</a></span><span class="op">(</span><span class="va">jv</span>, <span class="va">splicing_df</span><span class="op">)</span>                                 <span class="co"># Prepare the data       </span></span>
<span><span class="fu"><a href="reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">sv</span>,columns <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">'Nose_y_Central_Sitar'</span>,<span class="st">'Nose_y_Central_Tabla'</span><span class="op">)</span><span class="op">)</span> <span class="co"># Plot selected coordinates</span></span></code></pre></div>
<p><img src="reference/figures/README-ex6-1.png" width="70%"></p>
<p>Now we apply the Granger analysis to the segments and test how the musicians influence each other in each segment. The null hypothesis is that Tabla doesn’t influence Sitar and the p-values from the analysis are plotted as an indicator of the strength of the causality.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/granger_test.html">granger_test</a></span><span class="op">(</span><span class="va">sv</span>, </span>
<span>                  <span class="st">"Nose_y_Central_Sitar"</span>, <span class="st">"Nose_y_Central_Tabla"</span>, </span>
<span>                  lag <span class="op">=</span> <span class="fl">12</span><span class="op">/</span><span class="fl">25</span><span class="op">)</span>                                 <span class="co"># apply granger analysis</span></span>
<span><span class="fu"><a href="reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">g</span>, splicing_df <span class="op">=</span> <span class="va">splicing_df</span><span class="op">)</span>                         <span class="co"># show p-values (forward and backwards) </span></span></code></pre></div>
<p><img src="reference/figures/README-ex7-1.png" width="70%"></p>
<p>As we can see from the visualisation, there is not much causality taking place between the two performers as evidenced by the high p values. The last segments suggest a weak causality where sitar can be seen driving the tabla. The interpretation of these 30-second segments of the vertical head movement is challenging, especially when we know that the sitar player is the only musician producing sounds during these first 60-seconds of the opening alap. The tabla player may still be reacting to the performance of the sitar player and coordinating actions through performance cues (gaze) during the opening section.</p>
</div>
</div>
<div class="section level2">
<h2 id="wavelet-analysis">Wavelet analysis<a class="anchor" aria-label="anchor" href="#wavelet-analysis"></a>
</h2>
<p>Here we characterise the sitar player’s vertical movement periodicity with wavelet analysis constrained to 0.1 to 0.5 seconds. Note that the graphics output is driven by <a href="https://CRAN.R-project.org/package=WaveletComp" class="external-link"><code>waveletComp</code></a> package (see Roesch and Schmidbauer, 2018).</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">w</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/analyze_wavelet.html">analyze_wavelet</a></span><span class="op">(</span><span class="va">pv1</span>, <span class="st">"Nose_y"</span>, lowerPeriod <span class="op">=</span> <span class="fl">0.1</span>, upperPeriod <span class="op">=</span> <span class="fl">0.5</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span>, dj <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">25</span><span class="op">)</span></span>
<span><span class="co">#&gt;   |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%</span></span>
<span><span class="fu"><a href="reference/plot_power_spectrum.html">plot_power_spectrum</a></span><span class="op">(</span><span class="va">w</span>, <span class="va">pv1</span><span class="op">)</span></span></code></pre></div>
<p><img src="reference/figures/README-ex8-1.png" width="70%"></p>
<p>From this analysis we can summarise the periodicity over time.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/plot_wt_energy.html">plot_wt_energy</a></span><span class="op">(</span><span class="va">w</span>, <span class="va">pv1</span><span class="op">)</span></span></code></pre></div>
<p><img src="reference/figures/README-ex9-1.png" width="70%"></p>
<p>Or we can summarise the average power across frequency.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">maximum.level</span> <span class="op">&lt;-</span> <span class="fl">1.001</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">w</span><span class="op">$</span><span class="va">Power.avg</span><span class="op">)</span></span>
<span><span class="fu"><a href="reference/plot_average_power.html">plot_average_power</a></span><span class="op">(</span><span class="va">w</span>, <span class="va">pv1</span>, maximum.level <span class="op">=</span> <span class="va">maximum.level</span>, show.siglvl<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="reference/figures/README-ex10-1.png" width="70%"></p>
<p>For more examples, see <a href="https://tuomaseerola.github.io/movementsync/articles/movementsync.html" class="external-link">Quick Guide</a>.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li>Clayton, M., Leante, L., &amp; Tarsitani, S. (2021, April 15). <em>IEMP North Indian Raga</em>. <a href="doi:10.17605/OSF.IO/KS325" class="uri">doi:10.17605/OSF.IO/KS325</a>
</li>
<li>Clayton, M., Jakubowski, K., &amp; Eerola, T. (2019). Interpersonal entrainment in Indian instrumental music performance: Synchronization and movement coordination relate to tempo, dynamics, metrical and cadential structure. <em>Musicae Scientiae, 23(3)</em>, 304–331. <a href="doi:10.1177/1029864919844809" class="uri">doi:10.1177/1029864919844809</a>
</li>
<li>Eerola, T., Jakubowski, K., Moran, N., Keller, P., &amp; Clayton, M. (2018). Shared Periodic Performer Movements Coordinate Interactions in Duo Improvisations. <em>Royal Society Open Science, 5(2)</em>, 171520. <a href="doi:10.1098/rsos.171520" class="uri">doi:10.1098/rsos.171520</a>
</li>
<li>Roesch A., * Schmidbauer, H. (2018). <em>WaveletComp: Computational Wavelet Analysis</em>. R package version 1.1, <a href="https://CRAN.R-project.org/package=WaveletComp" class="external-link uri">https://CRAN.R-project.org/package=WaveletComp</a>.</li>
</ul>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=movementsync" class="external-link">View on CRAN</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li>
<a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a>
</li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing movementsync</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Tuomas Eerola <br><small class="roles"> Author, maintainer, copyright holder </small> <a href="https://orcid.org/0000-0002-2896-929X" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
<li>Martin Clayton <br><small class="roles"> Author </small> <a href="https://orcid.org/0000-0002-9670-5077" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
<li>Paul Emms <br><small class="roles"> Author </small>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental" class="external-link"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" alt="Lifecycle: experimental"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tuomas Eerola, Martin Clayton, Paul Emms.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
